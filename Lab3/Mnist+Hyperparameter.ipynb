{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MNIST DIGIT RECOGNITION TASK\n"
      ],
      "metadata": {
        "id": "qpuQwk98jjXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install numpy\n",
        "# pip install tensorflow\n",
        "# pip install keras\n",
        "# pip install pillow"
      ],
      "metadata": {
        "id": "VP7EyHOF1ZMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import libraries and dataset"
      ],
      "metadata": {
        "id": "7cgrV6eS1SJ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YB3hX7kE0lZD"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  to split the data of training and testing sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO38wAKk01X8",
        "outputId": "bfeb3a3f-51ab-4735-e82c-fa3be88537bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data Preprocessing"
      ],
      "metadata": {
        "id": "IdaO_VdF1hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_F84K5BoKC7",
        "outputId": "6355f28b-284e-46c7-d946-7f694eec7598"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# conversion of class vectors to matrices of  binary class\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "metadata": {
        "id": "bG2m5ox403f1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3erGFDupNBY",
        "outputId": "733dadcf-d50b-42fa-c16d-4c831fd13040"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "uc-a2ymu1kIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LvCb3oEZ08gI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model\n"
      ],
      "metadata": {
        "id": "Ou_CnQNE1nM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHBqWiGbrmP_",
        "outputId": "ff072308-75c6-4872-a4e8-5ef33b291ceb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               2359552   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2380938 (9.08 MB)\n",
            "Trainable params: 2380938 (9.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=3,validation_data=(x_test, y_test))\n",
        "print(\"The model has successfully trained\")\n",
        "model.save('mnist.h5')\n",
        "print(\"Saving the bot as mnist.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfPmI5AO1GEE",
        "outputId": "5e482acb-2625-4961-aca4-900afa8e98cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model has successfully trained\n",
            "Saving the bot as mnist.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "V8UK85NY1qYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8qPDXk-1I6a",
        "outputId": "e7e54906-7633-4e45-9a38-5fd5232e56e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.7536479830741882\n",
            "Test accuracy: 0.8349000215530396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of Hidden Units"
      ],
      "metadata": {
        "id": "lXWTHowN05Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
        "hidden_units_list = [32, 64, 128]  # Add more values as needed\n",
        "\n",
        "for hidden_units in hidden_units_list:\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hidden_units, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(2*hidden_units, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(4*hidden_units, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hidden_unit, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(lr=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining with {hidden_units} hidden units\")\n",
        "\n",
        "    hist = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=3,\n",
        "        validation_data=(x_test, y_test)\n",
        "    )\n",
        "\n",
        "    model.save(f\"mnist_hidden_units_{hidden_units}.h5\")\n",
        "    print(f\"Saving the model as mnist_hidden_units_{hidden_units}.h5\")\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f'Test loss: {score[0]}, Test accuracy: {score[1]}\\n')"
      ],
      "metadata": {
        "id": "0iA1Ada-1L9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc77e54-c0d1-40db-af55-416663a14a31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with 32 hidden units\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model as mnist_hidden_units_32.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.03196895122528076, Test accuracy: 0.9904000163078308\n",
            "\n",
            "\n",
            "Training with 64 hidden units\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "Saving the model as mnist_hidden_units_64.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.02582160010933876, Test accuracy: 0.9932000041007996\n",
            "\n",
            "\n",
            "Training with 128 hidden units\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "Saving the model as mnist_hidden_units_128.h5\n",
            "Test loss: 0.029209839180111885, Test accuracy: 0.991599977016449\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate"
      ],
      "metadata": {
        "id": "1XlO-0sW199X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have x_train, y_train, x_test, y_test defined for your dataset\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"\n",
        "    Learning rate scheduler for exponential decay.\n",
        "    \"\"\"\n",
        "    initial_lr = 0.001\n",
        "    k = 0.1\n",
        "    return initial_lr * np.exp(-k * epoch)\n",
        "\n",
        "# List of learning rate schedules to try\n",
        "lr_schedules = [\n",
        "    {'lr_type': 'constant_low', 'lr_value': 0.0001},\n",
        "    {'lr_type': 'constant_high', 'lr_value': 0.01},\n",
        "    {'lr_type': 'exponential_decay', 'lr_schedule': lr_schedule},\n",
        "    # Add more schedules as needed\n",
        "]\n",
        "\n",
        "epochs = 10  # You can adjust the total number of epochs as needed\n",
        "\n",
        "for schedule in lr_schedules:\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(lr=schedule['lr_value']),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining with learning rate schedule: {schedule}\")\n",
        "\n",
        "    if schedule['lr_type'] == 'exponential_decay':\n",
        "        lr_callback = LearningRateScheduler(schedule['lr_schedule'])\n",
        "        callbacks_list = [lr_callback]\n",
        "    else:\n",
        "        callbacks_list = []\n",
        "\n",
        "    hist = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=3,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=callbacks_list\n",
        "    )\n",
        "\n",
        "    print(\"The model has successfully trained\")\n",
        "\n",
        "    model.save(f\"mnist_lr_{schedule['lr_type']}_{schedule['lr_value']}.h5\")\n",
        "    print(f\"Saving the model as mnist_lr_{schedule['lr_type']}_{schedule['lr_value']}.h5\")\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f'Test loss: {score[0]}, Test accuracy: {score[1]}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "xIB0Htt23DCs",
        "outputId": "5eb31af2-cd9a-4563-b172-56d2b0eb9f58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate schedule: {'lr_type': 'constant_low', 'lr_value': 0.0001}\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model has successfully trained\n",
            "Saving the model as mnist_lr_constant_low_0.0001.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.02442564070224762, Test accuracy: 0.9925000071525574\n",
            "\n",
            "\n",
            "Training with learning rate schedule: {'lr_type': 'constant_high', 'lr_value': 0.01}\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model has successfully trained\n",
            "Saving the model as mnist_lr_constant_high_0.01.h5\n",
            "Test loss: 0.025184277445077896, Test accuracy: 0.992900013923645\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'lr_value'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bc587707185e>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     model.compile(\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     )\n",
            "\u001b[0;31mKeyError\u001b[0m: 'lr_value'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have x_train, y_train, x_test, y_test defined for your dataset\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"\n",
        "    Learning rate scheduler for exponential decay.\n",
        "    \"\"\"\n",
        "    initial_lr = 0.001  # You can adjust this as needed\n",
        "    k = 0.1  # You can adjust this as needed\n",
        "    return initial_lr * np.exp(-k * epoch)\n",
        "\n",
        "# Train the model with only exponential decay\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(lr=0.001),  # Initial learning rate (you can adjust this)\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nTraining with learning rate schedule: Exponential Decay\")\n",
        "\n",
        "lr_callback = LearningRateScheduler(lr_schedule)\n",
        "callbacks_list = [lr_callback]\n",
        "\n",
        "epochs = 10  # You can adjust the total number of epochs as needed\n",
        "\n",
        "hist = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=3,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "print(\"The model has successfully trained\")\n",
        "\n",
        "model.save(\"mnist_exponential_decay.h5\")\n",
        "print(\"Saving the model as mnist_exponential_decay.h5\")\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}, Test accuracy: {score[1]}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKwad_qF6z2Q",
        "outputId": "31b1fa6c-ce6b-45f6-f792-99838829b5b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate schedule: Exponential Decay\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model has successfully trained\n",
            "Saving the model as mnist_exponential_decay.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.024176474660634995, Test accuracy: 0.9934999942779541\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Width"
      ],
      "metadata": {
        "id": "PmFnG5Af3rbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have x_train, y_train, x_test, y_test defined for your dataset\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"\n",
        "    Learning rate scheduler for exponential decay.\n",
        "    \"\"\"\n",
        "    initial_lr = 0.001  # You can adjust this as needed\n",
        "    k = 0.1  # You can adjust this as needed\n",
        "    return initial_lr * np.exp(-k * epoch)\n",
        "\n",
        "# List of kernel sizes to try\n",
        "kernel_sizes = [(3, 3), (5, 5), (7, 7)]  # Add more sizes as needed\n",
        "\n",
        "epochs = 10  # You can adjust the total number of epochs as needed\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
        "    model.add(Conv2D(64, kernel_size=kernel_size, activation='relu'))\n",
        "    model.add(Conv2D(128, kernel_size=kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(lr=0.001),  # Initial learning rate (you can adjust this)\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining with kernel size: {kernel_size} and learning rate schedule: Exponential Decay\")\n",
        "\n",
        "    lr_callback = LearningRateScheduler(lr_schedule)\n",
        "    callbacks_list = [lr_callback]\n",
        "\n",
        "    hist = model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=3,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=callbacks_list\n",
        "    )\n",
        "\n",
        "    print(f\"The model with kernel size {kernel_size} has successfully trained\")\n",
        "\n",
        "    model.save(f\"mnist_kernel_{kernel_size}_exponential_decay.h5\")\n",
        "    print(f\"Saving the model as mnist_kernel_{kernel_size}_exponential_decay.h5\")\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f'Test loss: {score[0]}, Test accuracy: {score[1]}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN1pMzJW3uOC",
        "outputId": "b39072b0-15aa-4562-d764-9c0845fe0058"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with kernel size: (3, 3) and learning rate schedule: Exponential Decay\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model with kernel size (3, 3) has successfully trained\n",
            "Saving the model as mnist_kernel_(3, 3)_exponential_decay.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.02416018769145012, Test accuracy: 0.9944999814033508\n",
            "\n",
            "\n",
            "Training with kernel size: (5, 5) and learning rate schedule: Exponential Decay\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model with kernel size (5, 5) has successfully trained\n",
            "Saving the model as mnist_kernel_(5, 5)_exponential_decay.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.019375845789909363, Test accuracy: 0.9940999746322632\n",
            "\n",
            "\n",
            "Training with kernel size: (7, 7) and learning rate schedule: Exponential Decay\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model with kernel size (7, 7) has successfully trained\n",
            "Saving the model as mnist_kernel_(7, 7)_exponential_decay.h5\n",
            "Test loss: 0.018314827233552933, Test accuracy: 0.9947999715805054\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD Optimizer"
      ],
      "metadata": {
        "id": "bAh3x5JsCaTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have x_train, y_train, x_test, y_test defined for your dataset\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"\n",
        "    Learning rate scheduler for exponential decay.\n",
        "    \"\"\"\n",
        "    initial_lr = 0.001  # You can adjust this as needed\n",
        "    k = 0.1  # You can adjust this as needed\n",
        "    return initial_lr * np.exp(-k * epoch)\n",
        "\n",
        "# Kernel size for Conv2D layers\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "epochs = 10  # You can adjust the total number of epochs as needed\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, kernel_size=kernel_size, activation='relu'))\n",
        "model.add(Conv2D(128, kernel_size=kernel_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model with SGD optimizer and exponential decay learning rate\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=SGD(learning_rate=0.001),  # Initial learning rate (you can adjust this)\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining with kernel size: {kernel_size}, optimizer: SGD, and learning rate schedule: Exponential Decay\")\n",
        "\n",
        "lr_callback = LearningRateScheduler(lr_schedule)\n",
        "callbacks_list = [lr_callback]\n",
        "\n",
        "hist = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=3,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "print(f\"The model with kernel size {kernel_size}, optimizer: SGD, and exponential decay has successfully trained\")\n",
        "\n",
        "model.save(f\"mnist_kernel_{kernel_size}_sgd_exponential_decay.h5\")\n",
        "print(f\"Saving the model as mnist_kernel_{kernel_size}_sgd_exponential_decay.h5\")\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]}, Test accuracy: {score[1]}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQStzAw3CYPx",
        "outputId": "f9503312-8b35-4b8d-ff81-b84c8787104e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with kernel size: (3, 3), optimizer: SGD, and learning rate schedule: Exponential Decay\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "The model with kernel size (3, 3), optimizer: SGD, and exponential decay has successfully trained\n",
            "Saving the model as mnist_kernel_(3, 3)_sgd_exponential_decay.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.18833912909030914, Test accuracy: 0.9455000162124634\n",
            "\n"
          ]
        }
      ]
    }
  ]
}