{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znU7sMCxikIZ",
        "outputId": "9b78fdbc-1b39-46aa-da3d-f304e7c6a270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: (1734, 3)\n",
            "Validation data shape: (248, 3)\n",
            "Test data shape: (496, 3)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"D:\\Downloads\\hate_speech_dataset.csv\")\n",
        "\n",
        "# Shuffle the data\n",
        "data = shuffle(data, random_state=8012023)\n",
        "\n",
        "# Consider only 20 percent of the data\n",
        "subset_data = data.sample(frac=0.2, random_state=8012023)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "subset_data['class'] = label_encoder.fit_transform(subset_data['class'])\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_data, test_data = train_test_split(subset_data, test_size=0.2, random_state=8012023)\n",
        "train_data, valid_data = train_test_split(train_data, test_size=0.125, random_state=8012023)  # 10% of the original data for validation\n",
        "\n",
        "# Check the shapes of the resulting datasets\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Validation data shape:\", valid_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq0-5skqm8Fp",
        "outputId": "8c76b671-16bb-4192-dfdb-dfa77318837d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, Dropout, Concatenate, GlobalMaxPooling1D, Conv1D, Bidirectional, GRU\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFBertModel, BertTokenizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, GlobalMaxPooling1D, Conv1D, Bidirectional, GRU\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "# Combine all text data\n",
        "all_texts = list(train_data['tweet']) + list(valid_data['tweet']) + list(test_data['tweet'])\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize and pad sequences for train data\n",
        "train_encoded_dict = tokenizer(all_texts,\n",
        "                               add_special_tokens=True,\n",
        "                               truncation=True,\n",
        "                               max_length=None,\n",
        "                               padding='max_length',\n",
        "                               return_attention_mask=True,\n",
        "                               return_tensors='tf')\n",
        "\n",
        "# Extract the maximum sequence length from the training data\n",
        "max_length = train_encoded_dict['input_ids'].shape[1]\n",
        "\n",
        "# Tokenize and pad sequences for train data\n",
        "train_input_ids = train_encoded_dict['input_ids'][:len(train_data)]\n",
        "train_attention_masks = train_encoded_dict['attention_mask'][:len(train_data)]\n",
        "\n",
        "# Convert labels to TensorFlow tensors\n",
        "train_labels = tf.convert_to_tensor(train_data['class'].values)\n",
        "\n",
        "# Tokenize and pad sequences for validation data\n",
        "valid_encoded_dict = tokenizer(list(valid_data['tweet']),\n",
        "                               add_special_tokens=True,\n",
        "                               truncation=True,\n",
        "                               max_length=max_length,\n",
        "                               padding='max_length',\n",
        "                               return_attention_mask=True,\n",
        "                               return_tensors='tf')\n",
        "\n",
        "# Tokenize and pad sequences for validation data\n",
        "valid_input_ids = valid_encoded_dict['input_ids']\n",
        "valid_attention_masks = valid_encoded_dict['attention_mask']\n",
        "\n",
        "# Convert labels to TensorFlow tensors\n",
        "valid_labels = tf.convert_to_tensor(valid_data['class'].values)\n",
        "\n",
        "# Tokenize and pad sequences for test data\n",
        "test_encoded_dict = tokenizer(list(test_data['tweet']),\n",
        "                              add_special_tokens=True,\n",
        "                              truncation=True,\n",
        "                              max_length=max_length,\n",
        "                              padding='max_length',\n",
        "                              return_attention_mask=True,\n",
        "                              return_tensors='tf')\n",
        "\n",
        "# Tokenize and pad sequences for test data\n",
        "test_input_ids = test_encoded_dict['input_ids']\n",
        "test_attention_masks = test_encoded_dict['attention_mask']\n",
        "\n",
        "# Convert labels to TensorFlow tensors\n",
        "test_labels = tf.convert_to_tensor(test_data['class'].values)\n",
        "\n",
        "# Display the shapes of the processed data\n",
        "print(\"Train Input IDs shape:\", train_input_ids.shape)\n",
        "print(\"Train Attention Masks shape:\", train_attention_masks.shape)\n",
        "print(\"Train Labels shape:\", train_labels.shape)\n",
        "\n",
        "print(\"Validation Input IDs shape:\", valid_input_ids.shape)\n",
        "print(\"Validation Attention Masks shape:\", valid_attention_masks.shape)\n",
        "print(\"Validation Labels shape:\", valid_labels.shape)\n",
        "\n",
        "print(\"Test Input IDs shape:\", test_input_ids.shape)\n",
        "print(\"Test Attention Masks shape:\", test_attention_masks.shape)\n",
        "print(\"Test Labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1-JQ5Bs_pX",
        "outputId": "9eb91a1c-8ad1-4cc7-c018-10aff557efc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
            "                                                                                                  \n",
            " attention_masks (InputLaye  [(None, 512)]                0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel  multiple                     1094822   ['input_ids[0][0]',           \n",
            " )                                                        40         'attention_masks[0][0]']     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 510, 128)             295040    ['tf_bert_model[3][0]']       \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  (None, 512, 128)             320256    ['tf_bert_model[3][0]']       \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Gl  (None, 128)                  0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Gl  (None, 128)                  0         ['bidirectional_2[0][0]']     \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)        (None, 128)                  0         ['global_max_pooling1d_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)        (None, 128)                  0         ['global_max_pooling1d_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 256)                  0         ['dropout_41[0][0]',          \n",
            " )                                                                   'dropout_42[0][0]']          \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 128)                  32896     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)        (None, 128)                  0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 3)                    387       ['dropout_43[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 110130819 (420.12 MB)\n",
            "Trainable params: 110130819 (420.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, GlobalMaxPooling1D, Conv1D, Bidirectional, GRU\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "# Define BERT input layer\n",
        "input_ids = Input(shape=(train_encoded_dict['input_ids'].shape[1],), dtype=tf.int32, name='input_ids')\n",
        "attention_masks = Input(shape=(train_encoded_dict['attention_mask'].shape[1],), dtype=tf.int32, name='attention_masks')\n",
        "\n",
        "# BERT embedding layer\n",
        "bert_embedding = bert_model(input_ids, attention_mask=attention_masks)[0]\n",
        "\n",
        "# Channel 1: CNN\n",
        "cnn_layer = Conv1D(128, 3, activation='relu')(bert_embedding)\n",
        "cnn_layer = GlobalMaxPooling1D()(cnn_layer)\n",
        "cnn_layer = Dropout(0.2)(cnn_layer)\n",
        "\n",
        "# Channel 2: biGRU\n",
        "bigru_layer = Bidirectional(GRU(64, return_sequences=True))(bert_embedding)\n",
        "bigru_layer = GlobalMaxPooling1D()(bigru_layer)\n",
        "bigru_layer = Dropout(0.2)(bigru_layer)\n",
        "\n",
        "# Concatenate the outputs from both channels\n",
        "concatenated_features = Concatenate()([cnn_layer, bigru_layer])\n",
        "\n",
        "# Fully connected layer\n",
        "dense_layer = Dense(128, activation='relu')(concatenated_features)\n",
        "dense_layer = Dropout(0.2)(dense_layer)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(3, activation='softmax')(dense_layer)  # Assuming 3 classes for classification\n",
        "\n",
        "# Build the model\n",
        "model = Model(inputs=[input_ids, attention_masks], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-uRXQj9tv_c",
        "outputId": "0ce7ee2e-a87d-4006-aacf-8b09f8df88c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate F1 score\n",
        "def calculate_f1(precision, recall):\n",
        "    return 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    x=[train_encoded_dict['input_ids'][:len(train_data)], train_encoded_dict['attention_mask'][:len(train_data)]],\n",
        "    y=train_labels,\n",
        "    validation_data=([valid_encoded_dict['input_ids'], valid_encoded_dict['attention_mask']], valid_labels),\n",
        "    epochs=5,  # Adjust the number of epochs as needed\n",
        "    batch_size=32  # Adjust the batch size as needed\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "start_time = time.time()\n",
        "predictions = model.predict([test_encoded_dict['input_ids'], test_encoded_dict['attention_mask']])\n",
        "inference_time = (time.time() - start_time) / len(test_data)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = np.sum(predicted_labels == test_data['class'].values) / len(test_data)\n",
        "classification_report_result = classification_report(test_data['class'].values, predicted_labels, target_names=label_encoder.classes_, output_dict=True)\n",
        "confusion_matrix_result = confusion_matrix(test_data['class'].values, predicted_labels)\n",
        "\n",
        "# Display evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report_result)\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix_result)\n",
        "\n",
        "# Display training and inference times\n",
        "print(\"\\nTraining Time:\", training_time, \"seconds\")\n",
        "print(\"Inference Time per Sample:\", inference_time, \"seconds\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
