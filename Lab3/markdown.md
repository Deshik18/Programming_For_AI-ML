# Hyperparameter Tuning for CNN on MNIST Dataset

## Tasks:

1. **Learning Rate Variation:**
   - Trained the model with different learning rates, including constant (low and high) and exponentially decreasing rates.

2. **Kernel Size Variation:**
   - Explored the impact of different kernel sizes (3x3, 5x5, and 7x7) on the model's performance.

3. **SGD Optimizer and Exponential Decay Learning Rate:**
   - Implemented the Stochastic Gradient Descent (SGD) optimizer with an exponentially decreasing learning rate.

4. **Dropout Rate Variation:**
   - Explored the effect of different dropout rates on model training and performance.

5. **Data Augmentation and Training:**
   - Utilized data augmentation techniques during training to enhance model robustness.

6. **Model Evaluation:**
   - Evaluated model performance on the testing set using metrics such as test loss and accuracy.

7. **Model Saving:**
   - Saved each trained model with relevant filenames for later reference.

## Summary:

- Hyperparameter tuning involved adjusting learning rates, kernel sizes, and dropout rates to optimize model performance.
- Various configurations were explored to understand the impact of different hyperparameter values.
- The Stochastic Gradient Descent (SGD) optimizer and exponential decay learning rate were used for training.
- Data augmentation techniques were employed to enhance the diversity of the training dataset.
- Each trained model was saved for future reference.

The provided Markdown summary captures the key tasks and observations made during the hyperparameter tuning process.
